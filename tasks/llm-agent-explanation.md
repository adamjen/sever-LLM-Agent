+++
id = "llm-agent-explanation"
title = "LLM-Agent Tool Explanation"
context_type = "documentation"
status = "active"
last_updated = "2025-07-04"
tags = ["llm", "agent", "sirs", "documentation"]
+++

# Explanation of the LLM-Agent Tool

This document provides a comprehensive explanation of the `LLM-Agent` tool, detailing its functionality, how it works, and providing example prompts for interaction.

## What is the LLM-Agent?

The `LLM-Agent` is a sophisticated agent that combines a Large Language Model (LLM) for natural language processing with a SIRS (Symbolic, Interpretable, and Robust Systems) engine for probabilistic reasoning. This unique combination allows the agent to not only understand and generate human-like text but also to perform robust, probabilistic inference to provide more accurate and reliable responses.

The agent is accessible through a FastAPI service and a command-line interface (CLI).

## Key Functionalities

*   **Query Processing:** The agent can take a user's prompt, process it, and return a comprehensive response.
*   **Probabilistic Inference:** It leverages the SIRS engine to perform probabilistic inference based on the context of the query. This adds a layer of robust reasoning to the agent's capabilities.
*   **Session Management:** The agent can handle sessions, allowing for context to be maintained across multiple interactions.
*   **API and CLI Interaction:** Users can interact with the agent either programmatically via a REST API or directly through an interactive CLI.

## How It Works

The `LLM-Agent` follows a clear, orchestrated process:

1.  **Receives a Query:** A user sends a prompt to the agent, either through the API or the CLI.
2.  **SIRS Engine Inference:** The `AgentCore` component sends the context of the query to the SIRS engine. The SIRS engine runs its probabilistic model and returns an inference.
3.  **LLM Prompt Preparation:** The `AgentCore` then prepares a new, enriched prompt for the LLM. This prompt combines the original user prompt with the output from the SIRS engine.
4.  **LLM Query:** The enriched prompt is sent to the configured LLM (e.g., via LM Studio).
5.  **Response Generation:** The LLM generates a response based on the enriched prompt.
6.  **Final Response:** The `AgentCore` formats the LLM's output into a final response, which is then sent back to the user. This response includes the text generated by the LLM and a confidence score, which in a full implementation would be derived from the SIRS inference.

This entire workflow is exposed via the FastAPI application, which provides endpoints for status checks and querying the agent.

## Example Prompts

Here are a few example prompts you can use to interact with the `LLM-Agent`.

### Via API

You can send a `POST` request to the `/api/v1/agent/query` endpoint with a JSON body like this:

```json
{
  "session_id": "api-session-1",
  "prompt": "What are the chances of rain in London tomorrow, and what should I wear?",
  "context": "User is asking for a weather forecast and advice."
}
```

```json
{
  "session_id": "api-session-2",
  "prompt": "Explain the concept of photosynthesis in simple terms.",
  "context": "User is asking a general knowledge question."
}
```

### Via CLI

After starting the interactive chat with `python -m LLM-Agent.src.cli.cli chat`, you can simply type your prompts:

```
> What is the capital of France?
```

```
> I'm thinking of starting a new project using Python and FastAPI. What are the first steps I should take?